# 🛡️ Adversarial Robustness Playground 💥

A small AI project that explores how machine learning models can be tricked — and how we can make them stronger. Built with ❤️ using [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox), `scikit-learn`, `matplotlib`, and `numpy`.

## 🧠 What's inside?

This project lets you:
- Train models like logistic regression and random forests on multivariate data 🧬
- Launch adversarial attacks (e.g., FGSM) to simulate how bad actors can confuse your model 😈
- Visualize the impact of those attacks in 2D using PCA and see how robust models perform 📉➡️📈
- Try defenses using `AdversarialTrainer` from ART to build stronger classifiers 💪

## ✨ Key Features

- ✅ Clean training vs adversarial training comparison
- 📊 2D projections of attack effects (PCA)
- 🔍 Visual inspection of classification errors
- 🐍 Built with `numpy`, `scikit-learn`, `matplotlib`, and `ART`

## ❤️ For Asma

This project is a small gift for my favorite person. I miss you a lot and hope it helps with your final project! 🥰🌷

## 📁 Project Structure

```
.
├── kddcup.data_10_percent.gz  # Dataset file
├── kddcup.names               # Dataset description
├── adversarial_robustness_playground.ipynb  # Jupyter notebook
├── training_attack_types # Attack types
```



