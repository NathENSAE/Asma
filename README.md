# ğŸ›¡ï¸ Adversarial Robustness Playground ğŸ’¥

A small AI project that explores how machine learning models can be tricked â€” and how we can make them stronger. Built with â¤ï¸ using [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox), `scikit-learn`, `matplotlib`, and `numpy`.

## ğŸ§  What's inside?

This project lets you:
- Train models like logistic regression and random forests on multivariate data ğŸ§¬
- Launch adversarial attacks (e.g., FGSM) to simulate how bad actors can confuse your model ğŸ˜ˆ
- Visualize the impact of those attacks in 2D using PCA and see how robust models perform ğŸ“‰â¡ï¸ğŸ“ˆ
- Try defenses using `AdversarialTrainer` from ART to build stronger classifiers ğŸ’ª

## âœ¨ Key Features

- âœ… Clean training vs adversarial training comparison
- ğŸ“Š 2D projections of attack effects (PCA)
- ğŸ” Visual inspection of classification errors
- ğŸ Built with `numpy`, `scikit-learn`, `matplotlib`, and `ART`

## â¤ï¸ For Asma

This project is a small gift for my favorite person. I miss you a lot and hope it helps with your final project! ğŸ¥°ğŸŒ·

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ kddcup.data_10_percent.gz  # Dataset file
â”œâ”€â”€ kddcup.names               # Dataset description
â”œâ”€â”€ adversarial_robustness_playground.ipynb  # Jupyter notebook
â”œâ”€â”€ training_attack_types # Attack types
```



